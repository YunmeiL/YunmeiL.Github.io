---
layout: post
title: 4. Real-time Perceptually-enabled Task Guidance in the Extremes
subtitle: DARPA Project
categories: markdown
tags: [project]
---
<font size=3>
<p style="text-align:justify; text-justify:inter-ideograph;">
This a DARPA-funded project on the use of augmented reality for providing knowledge task guidance in extreme situations. The research challenge is to create a data-to-knowledge "pipeline" from wearable devices to an intelligent agency that can determine what information or guidance to provide to human operators in real-time. 
</p>
</font>

### 4.1 Project overview
<font size=3>
<p style="text-align:justify; text-justify:inter-ideograph;">

This project is to investigate augmented reality (AR) technologies for providing real-time knowledge-task guidance in extreme environments, including military helicopter pilot checklist completion and Army field-medic care for soldier amputee cases. Research requires application of formal task modeling approaches, including object-action identification with contingency states. User state modeling is to be conducted with real-time physiological measures and machine learning methods for state classification. User models are to be linked to AR interfaces to trigger presentation of task-relevant guidance during high-fidelity simulator trials. Guidance will be based on real-time execution of task simulation models in sync with experiment protocols being performed by actual human operators. Results will be used to support Army applications of AR devices.<br/><br/>

My responsibility in this project is to use feature engineering and ML techniques to predict operator cognitive workload based on real-time physiological signals collected by the wearable devices. 

</p>
</font>

### 4.2 Related papers
<font size=3>
<p style="text-align:justify; text-justify:inter-ideograph;">
In processing...
</p>
</font>

### 4.3 Demo video

![](https://youtu.be/6xAK8W69i-g)

